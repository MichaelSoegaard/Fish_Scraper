{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2df74997-ba30-4b34-84ac-bff362eed057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import re\n",
    "from random import randint\n",
    "import os\n",
    "#from GImageScraper import GoogleImageScraper\n",
    "from patch import webdriver_executable\n",
    "\n",
    "webdriver_path = \"./webdriver/chromedriver.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58ddce2a-ad68-4487-9796-97dbb76d46e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'd://fish_scraper/2/'\n",
    "\n",
    "fish_list = pd.read_csv('data/Fish_list.csv')\n",
    "fish_list = list(fish_list['Taxonomy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "235cf27b-85ba-4988-8c98-153fa2581b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_list = fish_list[4:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ae669f3-2250-4498-a081-f4a571addca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_string = [string.replace(\" \", \"-\") for string in fish_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49c7a36b-c176-43f1-a3d1-4ca5570b451f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Brochis britskii', 'Brochis splendens']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fish_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "beab120f-38f0-4985-a976-7d8fc75dfb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import time\n",
    "\n",
    "class GoogleImageScraper():\n",
    "    def __init__(self,webdriver_path,image_path, search_key=\"cat\",number_of_images=1,headless=False,min_resolution=(0,0),max_resolution=(1920,1080), verbose=False):\n",
    "        #check parameter types\n",
    "        image_path = os.path.join(image_path, search_key)\n",
    "        if (type(number_of_images)!=int):\n",
    "            print(\"[Error] Number of images must be integer value.\")\n",
    "            return\n",
    "        if not os.path.exists(image_path):\n",
    "            print(\"[INFO] Image path not found. Creating a new folder.\")\n",
    "            os.makedirs(image_path)\n",
    "        #check if chromedriver is updated\n",
    "        while(True):\n",
    "            try:\n",
    "                #try going to www.google.com\n",
    "                options = Options()\n",
    "                if(headless):\n",
    "                    options.add_argument('--headless')\n",
    "                driver = webdriver.Chrome(webdriver_path, chrome_options=options)\n",
    "                driver.set_window_size(1400,1050)\n",
    "                driver.get(\"https://www.google.com\")\n",
    "                break\n",
    "            except:\n",
    "                #patch chromedriver if not available or outdated\n",
    "                try:\n",
    "                    driver\n",
    "                except NameError:\n",
    "                    is_patched = patch.download_lastest_chromedriver()\n",
    "                else:\n",
    "                    is_patched = patch.download_lastest_chromedriver(driver.capabilities['version'])\n",
    "                if (not is_patched): \n",
    "                    exit(\"[ERR] Please update the chromedriver.exe in the webdriver folder according to your chrome version:https://chromedriver.chromium.org/downloads\")\n",
    "                    \n",
    "        self.driver = driver\n",
    "        self.search_key = search_key\n",
    "        self.number_of_images = number_of_images\n",
    "        self.webdriver_path = webdriver_path\n",
    "        self.image_path = image_path\n",
    "        self.url = \"https://www.google.com/search?q=%s&source=lnms&tbm=isch&sa=X&ved=2ahUKEwie44_AnqLpAhUhBWMBHUFGD90Q_AUoAXoECBUQAw&biw=1920&bih=947\"%(search_key)\n",
    "        self.headless=headless\n",
    "        self.min_resolution = min_resolution\n",
    "        self.max_resolution = max_resolution\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def find_image_urls(self):\n",
    "        \"\"\"\n",
    "            This function search and return a list of image urls based on the search key.\n",
    "            Example:\n",
    "                google_image_scraper = GoogleImageScraper(\"webdriver_path\",\"image_path\",\"search_key\",number_of_photos)\n",
    "                image_urls = google_image_scraper.find_image_urls()\n",
    "                \n",
    "        \"\"\"\n",
    "        print(\"[INFO] Scraping for image link... Please wait.\")\n",
    "        image_urls=[]\n",
    "        count = 0\n",
    "        missed_count = 0\n",
    "        self.driver.get(self.url)\n",
    "        time.sleep(3)\n",
    "        indx = 1\n",
    "        while self.number_of_images > count:\n",
    "            try:\n",
    "                #find and click image\n",
    "                imgurl = self.driver.find_element(By.XPATH, '//*[@id=\"islrg\"]/div[1]/div[%s]/a[1]/div[1]/img'%(str(indx)))\n",
    "                imgurl.click()\n",
    "                missed_count = 0\n",
    "            except Exception:\n",
    "                print(\"[-] Unable to click this photo.\")\n",
    "                missed_count = missed_count + 1\n",
    "                if (missed_count>10):\n",
    "                    print(\"[INFO] No more photos.\")\n",
    "                    break\n",
    "                 \n",
    "            try:\n",
    "                #select image from the popup\n",
    "                time.sleep(2)\n",
    "                class_names = [\"n3VNCb\"]\n",
    "                images = [self.driver.find_elements(by=By.CLASS_NAME, value=class_name) for class_name in class_names if len(self.driver.find_elements(by=By.CLASS_NAME, value=class_name)) != 0 ][0]\n",
    "                #only download images that starts with http\n",
    "                for image in images:\n",
    "                    #only download images that starts with http\n",
    "                    src_link = image.get_attribute(\"src\")\n",
    "                    if((\"http\" in  src_link) and (not \"encrypted\" in src_link)):\n",
    "                        if self.verbose:\n",
    "                            print(\"[INFO] %d. %s\"%(count,src_link))\n",
    "                        image_urls.append(src_link)\n",
    "                        count +=1\n",
    "                        break\n",
    "            except Exception:\n",
    "                print(\"[INFO] Unable to get link\")   \n",
    "                \n",
    "            try:\n",
    "                #scroll page to load next image\n",
    "                if(count%3==0):\n",
    "                    self.driver.execute_script(\"window.scrollTo(0, \"+str(indx*60)+\");\")\n",
    "                element = self.driver.find_element(by=By.CLASS_NAME, value=\"mye4qd\")\n",
    "                element.click()\n",
    "                print(\"[INFO] Loading more photos\")\n",
    "                time.sleep(3)\n",
    "            except Exception:  \n",
    "                time.sleep(1)\n",
    "            indx += 1\n",
    "\n",
    "        \n",
    "        self.driver.quit()\n",
    "        print(\"[INFO] Google search ended\")\n",
    "        return image_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0828f595-3012-4233-87c7-8d0226c57afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dl_image(fish, image_path, url):\n",
    "    \n",
    "    fileid = randint(10000, 99999)\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    content_type = r.headers['content-type']\n",
    "    extension = mimetypes.guess_extension(content_type)    \n",
    "    open(f\"{image_path}{fish}/{fish[:3]}{str(fileid)}{extension}\", 'wb').write(r.content)\n",
    "\n",
    "def scrape_fishbase(fish, image_path):\n",
    "    \n",
    "    if not os.path.exists(image_path+fish):\n",
    "        os.makedirs(image_path+fish)\n",
    "    \n",
    "    def get_url(image):\n",
    "        url = (image['src'])\n",
    "        url = url.replace(\"%2F\", \"/\")\n",
    "        url = \"https://www.fishbase.de\"+url[2:]\n",
    "        return url  \n",
    "    \n",
    "    html_page = requests.get(f'http://fishbase.de/summary/{fish}')\n",
    "    soup = bs(html_page.content, 'html.parser')\n",
    "    txt = soup.find(title=\"English\")\n",
    "    fish_id = re.search('[0-9]+', str(txt))[0]\n",
    "\n",
    "    html_page = requests.get(f'https://www.fishbase.de/photos/thumbnailssummary.php?ID={fish_id}')\n",
    "    soup = bs(html_page.content, 'html.parser')\n",
    "    image_urls = soup.find_all(\"img\", width=\"300\")\n",
    "    urls = [get_url(image) for image in image_urls]\n",
    "    urls = list(set(urls))\n",
    "    [dl_image(fish, image_path, url) for url in urls]\n",
    "    print(f\"Fishbase done scraping for {fish}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31aeb6c7-da4c-48c5-92e6-20724d4cd4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_google(fish, image_path=image_path):\n",
    "\n",
    "    number_of_images = 200\n",
    "    headless = False\n",
    "    min_resolution=(100,100)\n",
    "    max_resolution=(1400,1200)\n",
    "\n",
    "    image_scrapper = GoogleImageScraper(webdriver_path,image_path,fish,number_of_images,headless,min_resolution,max_resolution)\n",
    "    urls = image_scrapper.find_image_urls()\n",
    "    i = 1\n",
    "    for url in urls:\n",
    "        print(f\"Round {i} of {len(urls)}\")\n",
    "        try:\n",
    "            dl_image(fish, image_path, url)\n",
    "        except Exception as e:\n",
    "            print('Ignoring Exception', e)\n",
    "        i += 1\n",
    "    #[dl_image(fish, url) for url in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe934f28-0643-427a-9c6b-15ca4a51b198",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brochis-britskii\n",
      "Fishbase done scraping for Brochis-britskii\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael\\AppData\\Local\\Temp/ipykernel_34232/4080853171.py:26: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(webdriver_path, chrome_options=options)\n",
      "C:\\Users\\Michael\\AppData\\Local\\Temp/ipykernel_34232/4080853171.py:26: DeprecationWarning: use options instead of chrome_options\n",
      "  driver = webdriver.Chrome(webdriver_path, chrome_options=options)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Scraping for image link... Please wait.\n",
      "[-] Unable to click this photo.\n",
      "[-] Unable to click this photo.\n",
      "[-] Unable to click this photo.\n",
      "[-] Unable to click this photo.\n",
      "[-] Unable to click this photo.\n",
      "[-] Unable to click this photo.\n",
      "[-] Unable to click this photo.\n",
      "[-] Unable to click this photo.\n",
      "[INFO] Google search ended\n",
      "Round 1 of 200\n",
      "Round 2 of 200\n",
      "Round 3 of 200\n",
      "Round 4 of 200\n",
      "Round 5 of 200\n",
      "Round 6 of 200\n",
      "Round 7 of 200\n",
      "Round 8 of 200\n",
      "Round 9 of 200\n",
      "Round 10 of 200\n",
      "Round 11 of 200\n",
      "Round 12 of 200\n",
      "Round 13 of 200\n",
      "Round 14 of 200\n",
      "Round 15 of 200\n",
      "Round 16 of 200\n",
      "Round 17 of 200\n",
      "Round 18 of 200\n",
      "Round 19 of 200\n",
      "Round 20 of 200\n",
      "Round 21 of 200\n",
      "Round 22 of 200\n",
      "Round 23 of 200\n",
      "Round 24 of 200\n",
      "Round 25 of 200\n",
      "Round 26 of 200\n",
      "Round 27 of 200\n",
      "Round 28 of 200\n",
      "Round 29 of 200\n",
      "Round 30 of 200\n",
      "Round 31 of 200\n",
      "Round 32 of 200\n",
      "Round 33 of 200\n",
      "Round 34 of 200\n",
      "Round 35 of 200\n",
      "Round 36 of 200\n",
      "Round 37 of 200\n",
      "Round 38 of 200\n",
      "Round 39 of 200\n",
      "Round 40 of 200\n",
      "Round 41 of 200\n",
      "Round 42 of 200\n",
      "Round 43 of 200\n",
      "Round 44 of 200\n",
      "Round 45 of 200\n",
      "Round 46 of 200\n",
      "Round 47 of 200\n",
      "Round 48 of 200\n",
      "Round 49 of 200\n",
      "Round 50 of 200\n",
      "Round 51 of 200\n",
      "Round 52 of 200\n",
      "Round 53 of 200\n",
      "Round 54 of 200\n",
      "Round 55 of 200\n",
      "Round 56 of 200\n",
      "Round 57 of 200\n",
      "Round 58 of 200\n",
      "Round 59 of 200\n",
      "Round 60 of 200\n",
      "Round 61 of 200\n",
      "Round 62 of 200\n",
      "Round 63 of 200\n",
      "Round 64 of 200\n",
      "Round 65 of 200\n",
      "Round 66 of 200\n",
      "Round 67 of 200\n",
      "Ignoring Exception HTTPSConnectionPool(host='www.discoverlife.org', port=443): Max retries exceeded with url: /nh/maps/Vertebrata/Fish/Callichthyidae/Brochis/map_of_Brochis_britskii.jpg (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')))\n",
      "Round 68 of 200\n",
      "Round 69 of 200\n",
      "Round 70 of 200\n",
      "Round 71 of 200\n",
      "Round 72 of 200\n",
      "Round 73 of 200\n",
      "Round 74 of 200\n",
      "Round 75 of 200\n",
      "Round 76 of 200\n",
      "Round 77 of 200\n",
      "Round 78 of 200\n",
      "Round 79 of 200\n",
      "Round 80 of 200\n",
      "Round 81 of 200\n",
      "Round 82 of 200\n",
      "Round 83 of 200\n",
      "Round 84 of 200\n",
      "Round 85 of 200\n",
      "Round 86 of 200\n",
      "Round 87 of 200\n",
      "Round 88 of 200\n",
      "Round 89 of 200\n",
      "Round 90 of 200\n",
      "Round 91 of 200\n",
      "Round 92 of 200\n",
      "Round 93 of 200\n",
      "Round 94 of 200\n",
      "Round 95 of 200\n",
      "Round 96 of 200\n",
      "Round 97 of 200\n",
      "Round 98 of 200\n",
      "Round 99 of 200\n",
      "Round 100 of 200\n",
      "Round 101 of 200\n",
      "Round 102 of 200\n",
      "Round 103 of 200\n",
      "Round 104 of 200\n",
      "Round 105 of 200\n",
      "Round 106 of 200\n",
      "Round 107 of 200\n",
      "Round 108 of 200\n",
      "Round 109 of 200\n",
      "Round 110 of 200\n",
      "Round 111 of 200\n",
      "Round 112 of 200\n",
      "Round 113 of 200\n",
      "Round 114 of 200\n",
      "Round 115 of 200\n",
      "Round 116 of 200\n",
      "Round 117 of 200\n",
      "Round 118 of 200\n",
      "Round 119 of 200\n",
      "Round 120 of 200\n",
      "Round 121 of 200\n",
      "Round 122 of 200\n",
      "Round 123 of 200\n",
      "Round 124 of 200\n",
      "Round 125 of 200\n",
      "Round 126 of 200\n",
      "Round 127 of 200\n",
      "Round 128 of 200\n",
      "Round 129 of 200\n",
      "Round 130 of 200\n",
      "Round 131 of 200\n",
      "Round 132 of 200\n",
      "Round 133 of 200\n",
      "Round 134 of 200\n",
      "Round 135 of 200\n",
      "Round 136 of 200\n",
      "Round 137 of 200\n",
      "Round 138 of 200\n",
      "Round 139 of 200\n",
      "Round 140 of 200\n",
      "Round 141 of 200\n",
      "Round 142 of 200\n",
      "Round 143 of 200\n",
      "Round 144 of 200\n",
      "Round 145 of 200\n",
      "Round 146 of 200\n",
      "Round 147 of 200\n",
      "Round 148 of 200\n",
      "Round 149 of 200\n",
      "Round 150 of 200\n",
      "Round 151 of 200\n",
      "Round 152 of 200\n",
      "Round 153 of 200\n",
      "Round 154 of 200\n",
      "Round 155 of 200\n",
      "Round 156 of 200\n",
      "Round 157 of 200\n",
      "Round 158 of 200\n",
      "Round 159 of 200\n",
      "Round 160 of 200\n",
      "Round 161 of 200\n",
      "Round 162 of 200\n",
      "Round 163 of 200\n",
      "Round 164 of 200\n",
      "Round 165 of 200\n",
      "Round 166 of 200\n",
      "Round 167 of 200\n",
      "Round 168 of 200\n",
      "Round 169 of 200\n",
      "Round 170 of 200\n",
      "Round 171 of 200\n",
      "Round 172 of 200\n",
      "Round 173 of 200\n",
      "Round 174 of 200\n",
      "Round 175 of 200\n",
      "Round 176 of 200\n",
      "Round 177 of 200\n",
      "Round 178 of 200\n",
      "Round 179 of 200\n",
      "Round 180 of 200\n",
      "Round 181 of 200\n",
      "Round 182 of 200\n",
      "Round 183 of 200\n",
      "Round 184 of 200\n",
      "Round 185 of 200\n",
      "Round 186 of 200\n",
      "Round 187 of 200\n",
      "Round 188 of 200\n",
      "Round 189 of 200\n",
      "Round 190 of 200\n",
      "Round 191 of 200\n",
      "Round 192 of 200\n",
      "Round 193 of 200\n",
      "Round 194 of 200\n",
      "Round 195 of 200\n",
      "Round 196 of 200\n",
      "Round 197 of 200\n",
      "Round 198 of 200\n",
      "Round 199 of 200\n",
      "Round 200 of 200\n",
      "Brochis-splendens\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'd://fish_scraper/Brochis-splendens/Bro56521.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_34232/195293539.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfish\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfish_string\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfish\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mscrape_fishbase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfish\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mscrape_google\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfish\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_34232/2842348631.py\u001b[0m in \u001b[0;36mscrape_fishbase\u001b[1;34m(fish, image_path)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0murls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mget_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mimage_urls\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0murls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[1;33m[\u001b[0m\u001b[0mdl_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfish\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0murls\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Fishbase done scraping for {fish}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_34232/2842348631.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0murls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mget_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mimage_urls\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0murls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[1;33m[\u001b[0m\u001b[0mdl_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfish\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0murls\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Fishbase done scraping for {fish}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_34232/2842348631.py\u001b[0m in \u001b[0;36mdl_image\u001b[1;34m(fish, url)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mfileid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m99999\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"d://fish_scraper/{fish}/{fish[:3]}{str(fileid)}.jpg\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mscrape_fishbase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfish\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'd://fish_scraper/Brochis-splendens/Bro56521.jpg'"
     ]
    }
   ],
   "source": [
    "for fish in fish_string:\n",
    "    print(fish)\n",
    "    scrape_fishbase(fish, image_path)\n",
    "    scrape_google(fish, image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd2cd6c-4342-4605-b968-e5b8f19cf793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_many_hdf5(images, labels):\n",
    "    \"\"\" Stores an array of images to HDF5.\n",
    "        Parameters:\n",
    "        ---------------\n",
    "        images       images array, (N, 32, 32, 3) to be stored\n",
    "        labels       labels array, (N, 1) to be stored\n",
    "    \"\"\"\n",
    "    num_images = len(images)\n",
    "\n",
    "    # Create a new HDF5 file\n",
    "    file = h5py.File(hdf5_dir / f\"{num_images}_many.h5\", \"w\")\n",
    "\n",
    "    # Create a dataset in the file\n",
    "    dataset = file.create_dataset(\n",
    "        \"images\", np.shape(images), h5py.h5t.STD_U8BE, data=images\n",
    "    )\n",
    "    meta_set = file.create_dataset(\n",
    "        \"meta\", np.shape(labels), h5py.h5t.STD_U8BE, data=labels\n",
    "    )\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e33021-d535-4490-9d6b-083aedcf902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, urllib.parse\n",
    "from selenium import webdriver\n",
    "\n",
    "driver = webdriver.Chrome(executable_path=webdriver_path)\n",
    "\n",
    "for result in driver.find_elements_by_css_selector('.js-images-link'):\n",
    "    title = result.find_element_by_css_selector('.js-images-link a img').get_attribute('alt')\n",
    "    link = result.find_element_by_css_selector('.js-images-link a').get_attribute('href')\n",
    "    thumbnail_encoded = result.find_element_by_css_selector('.js-images-link a img').get_attribute('src')\n",
    "    # https://regex101.com/r/4pgG5m/1\n",
    "    match_thumbnail_urls = ''.join(re.findall(r'https\\:\\/\\/external\\-content\\.duckduckgo\\.com\\/iu\\/\\?u\\=(.*)&f=1', thumbnail_encoded))\n",
    "    # https://www.kite.com/python/answers/how-to-decode-a-utf-8-url-in-python\n",
    "    thumbnail = urllib.parse.unquote(match_thumbnail_urls).replace('&h=160', '')\n",
    "    image = result.get_attribute('data-id')\n",
    "\n",
    "    print(f'{title}\\n{link}\\n{thumbnail}\\n{image}\\n')\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25643fb0-ba68-46c3-9eec-779b9bed97a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#driver = webdriver.Chrome(webdriver_path, chrome_options=options)\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "options = Options()\n",
    "options.add_argument('--headless')\n",
    "\n",
    "driver = webdriver.Chrome(webdriver_path, options=options)\n",
    "\n",
    "indx=1\n",
    "search_key = fish\n",
    "image_urls = []\n",
    "for i in range(1,20):\n",
    "    url = \"https://www.google.com/search?q=%s&source=lnms&tbm=isch&sa=X&ved=2ahUKEwie44_AnqLpAhUhBWMBHUFGD90Q_AUoAXoECBUQAw&biw=1920&bih=947\"%(search_key)\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    imgurl = driver.find_element(By.XPATH, value=f'//*[@id=\"islrg\"]/div[1]/div[{i}]/a[1]/div[1]/img')\n",
    "    \n",
    "    imgurl.click()\n",
    "    time.sleep(1)\n",
    "    class_names = [\"n3VNCb\"]\n",
    "    images = [driver.find_elements(by=By.CLASS_NAME, value=class_name) for class_name in class_names if len(driver.find_elements(by=By.CLASS_NAME, value=class_name)) != 0 ][0]\n",
    "    \n",
    "    for image in images:\n",
    "    #only download images that starts with http\n",
    "        src_link = image.get_attribute(\"src\")\n",
    "        if((\"http\" in  src_link) and (not \"encrypted\" in src_link)):\n",
    "            image_urls.append(src_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569f3d03-203a-404e-9a83-95b7886bd336",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d2b570-88d4-4c84-a99d-d84b392d73ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xx = driver.find_elements(by=By.CLASS_NAME, value='n3VNCb')\n",
    "#src_link = xx[0].get_attribute(\"src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3af6a47-c1de-4a05-9941-630f3235b829",
   "metadata": {},
   "outputs": [],
   "source": [
    "fish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7ebd60-f1ac-4a78-a245-d6f4965afc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "[dl_image(url, fish) for url in image_urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5cf2af8-3780-49e5-a658-e3520e99453d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".jpg\n"
     ]
    }
   ],
   "source": [
    "import mimetypes\n",
    "\n",
    "r = requests.get(\"https://www.ikea.com/dk/da/images/products/jaettestor-tojdyr-elefant-gra__0877889_pe633605_s5.jpg\", allow_redirects=True)\n",
    "content_type = r.headers['content-type']\n",
    "extension = mimetypes.guess_extension(content_type)\n",
    "print(extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60910fc2-6d30-4096-8ef7-ebd29e255f5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python310",
   "language": "python",
   "name": "python310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
