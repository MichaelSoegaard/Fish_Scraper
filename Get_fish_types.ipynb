{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca602dca-305f-43ec-87d7-5209fdbb1660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from Fishy import fish_scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13ebe21d-746f-486b-b91f-67f816ad6204",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = \"d://fish_scraper/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3002cab-dc4b-47d1-9aed-6e6f0fb62205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "wikiurl=\"https://en.wikipedia.org/wiki/List_of_freshwater_aquarium_fish_species\"\n",
    "table_class=\"sortable\"\n",
    "response=requests.get(wikiurl)\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bd8cb85-07a2-43d5-8c0e-1703e9e8f1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs(response.text, 'html.parser')\n",
    "fish_tables = soup.findAll('table',{'class':table_class}) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7239cb2-9016-46a7-bbe5-f1363511ae8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tables = pd.read_html(str(fish_tables))\n",
    "\n",
    "full_df = pd.DataFrame(columns=['Common name', 'Taxonomy'])\n",
    "\n",
    "for table in all_tables:\n",
    "    cols = table.columns.tolist()\n",
    "    if 'Common name' not in cols:\n",
    "        continue\n",
    "    t = table[['Common name', 'Taxonomy']]\n",
    "    full_df = full_df.append(t, ignore_index=True)\n",
    "\n",
    "def clean_string(fish_str):\n",
    "    pattern = r'[' + string.punctuation + ']'\n",
    "    fish_str = re.sub(pattern, '', fish_str)\n",
    "    fish_str.replace(\" \", \"-\")\n",
    "    return fish_str\n",
    "    \n",
    "    \n",
    "full_df['Taxonomy'] = full_df['Taxonomy'].apply(lambda x: clean_string(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5701aca-d963-414c-9dc0-5e2c59aeef44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df['Taxonomy'].isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b95649-314b-4628-8d72-9df1957bbe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "    \n",
    "full_df.to_csv('data/Fish_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa09c98-214b-408c-85ef-a21af4f0894f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fish(fish, image_folder):\n",
    "    f = fish_scraper(fish, image_folder, headless=False, num_images=200, max_res=(1400,1200), export_urls=True)\n",
    "    f.run()\n",
    "\n",
    "Parallel(n_jobs=8)(delayed(get_fish)(fish, image_folder) for fish in full_df['Taxonomy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9269b28c-394a-48b0-a9eb-861c1c6cfa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "headless = False\n",
    "\n",
    "#fish = \"Callichthys-callichthys\"\n",
    "webdriver_path=\"./webdriver/chromedriver.exe\"\n",
    "url = (\"https://www.google.com/search?q=%s&source=lnms&tbm=isch&sa=X&ved=2ahUKEwie44_AnqLpAhUhBWMBHUFGD90Q_AUoAXoECBUQAw&biw=1920&bih=947\"% (fish))\n",
    "options = Options()\n",
    "if headless:\n",
    "    options.add_argument(\"--headless\")\n",
    "driver = webdriver.Chrome(webdriver_path, chrome_options=options)\n",
    "driver.set_window_size(1400, 1050)\n",
    "driver.get(\"https://www.google.com\")\n",
    "\n",
    "count = 0\n",
    "missed_count = 0\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "indx = 1\n",
    "verbose = True\n",
    "urls = []\n",
    "fishes = list(full_df['Taxonomy'])\n",
    "\n",
    "for fish in fishes[:5]:\n",
    "    for i in range(20):\n",
    "        imgurl = driver.find_element(By.XPATH, '//*[@id=\"islrg\"]/div[1]/div[%s]/a[1]/div[1]/img' % (str(indx)),)\n",
    "        imgurl.click()\n",
    "        time.sleep(2)\n",
    "\n",
    "        class_names = [\"n3VNCb\"] #[\"v4dQwb\"] #\n",
    "        images = [driver.find_elements(by=By.CLASS_NAME, value=class_name)\n",
    "        for class_name in class_names if len(driver.find_elements(by=By.CLASS_NAME, value=class_name))!= 0][0]\n",
    "                        # only download images that starts with http\n",
    "        for image in images:\n",
    "                            # only download images that starts with http\n",
    "            src_link = image.get_attribute(\"src\")\n",
    "            if (\"http\" in src_link) and (not \"encrypted\" in src_link):\n",
    "                if verbose:\n",
    "                    print(\"[INFO] %d. %s\" % (count, src_link))\n",
    "                urls.append(src_link)\n",
    "                count += 1\n",
    "                break\n",
    "\n",
    "        if count % 3 == 0:\n",
    "            driver.execute_script(\"window.scrollTo(0, \" + str(indx * 60) + \");\")\n",
    "        element = driver.find_element(by=By.CLASS_NAME, value=\"lxa62b.MIdC8d.jwwPNd\") # mye4qd\n",
    "        element.click()\n",
    "        print(\"[INFO] Loading more photos\")\n",
    "        time.sleep(3)\n",
    "self.driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4a3b93-97a8-4ac6-b504-cae0dd46c63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "\n",
    "#fish = \"Callichthys-callichthys\"\n",
    "\n",
    "for fish in full_df['Taxonomy']:\n",
    "    html_page = requests.get(f\"http://fishbase.de/summary/{fish}\")\n",
    "    soup = bs(html_page.content, \"html.parser\")\n",
    "\n",
    "    txt = soup.find(title=\"English\")\n",
    "    if txt == None:\n",
    "        continue\n",
    "    fish_id = re.search(\"[0-9]+\", str(txt))[0]\n",
    "    print(f\"{fish} - {fish_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7733e5a-a6b1-44cb-a90a-b7d73295b581",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2ea81c-d9cf-404a-a697-c35f0c2fc740",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df['Taxonomy'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c6abfd-ff84-4994-b427-888faf41b5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"d:/fish_scraper/2/fish_img_urls.json\") as f:\n",
    "    url_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc22abda-7fbf-47ee-bf6a-7c65cfc6ebca",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec08fc4-28e3-4eaa-bbac-5cd969398692",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = soup.find(class_='slabel8') #\n",
    "txt = soup.find_all('a')\n",
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbb2047-e72b-4d81-91ca-abe887149ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dd45ed-f071-4e18-bf61-17ff300add8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3257c66f-3988-48f7-a848-9a969e5b87f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(597, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1696bfa4-8980-4b2c-b099-033c7e9148cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
